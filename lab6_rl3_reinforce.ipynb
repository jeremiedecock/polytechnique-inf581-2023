{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Policy Gradient\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2023/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2023](https://moodle.polytechnique.fr/course/view.php?id=14259) Lab session #6\n",
    "\n",
    "2019-2023 Jérémie Decock, Mohamed Alami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2023/blob/master/lab6_rl3_reinforce.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2023/master?filepath=lab6_rl3_reinforce.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf581-2023/blob/master/lab6_rl3_reinforce.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2023/raw/master/lab6_rl3_reinforce.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: *PyTorch*, *Gymnasium*, NumPy, Pandas, Seaborn, imageio, pygame, ipywidgets, ipython and tqdm.\n",
    "\n",
    "You can install them with the following command (the next cells do this for you if you use the Google Colab environment):\n",
    "\n",
    "```\n",
    "pip install gymnasium, imageio, ipython, ipywidgets, nnfigs, numpy, pandas, pygame, seaborn, torch, tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install nnfigs inf581\".split(), stdout=subprocess.PIPE)\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "    from inf581 import *\n",
    "\n",
    "from inf581.lab7 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import Image   # To display GIF images in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\U{V}\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "%\n",
    "\\def\\E{\\mathbb{E}}\n",
    "%\\newcommand{transition}{T(s,a,s')}\n",
    "%\\newcommand{transitionfunc}{\\mathcal{T}^a_{ss'}}\n",
    "\\newcommand{transitionfunc}{P}\n",
    "\\newcommand{transitionfuncinst}{P(\\nextstate|\\state,\\action)}\n",
    "\\newcommand{transitionfuncpi}{\\mathcal{T}^{\\pi_i(s)}_{ss'}}\n",
    "\\newcommand{rewardfunc}{r}\n",
    "\\newcommand{rewardfuncinst}{r(\\state,\\action,\\nextstate)}\n",
    "\\newcommand{rewardfuncpi}{r(s,\\pi_i(s),s')}\n",
    "\\newcommand{statespace}{\\mathcal{S}}\n",
    "\\newcommand{statespaceterm}{\\mathcal{S}^F}\n",
    "\\newcommand{statespacefull}{\\mathcal{S^+}}\n",
    "\\newcommand{actionspace}{\\mathcal{A}}\n",
    "\\newcommand{reward}{R}\n",
    "\\newcommand{statet}{S}\n",
    "\\newcommand{actiont}{A}\n",
    "\\newcommand{newstatet}{S'}\n",
    "\\newcommand{nextstate}{\\state'}\n",
    "\\newcommand{newactiont}{A'}\n",
    "\\newcommand{stepsize}{\\alpha}\n",
    "\\newcommand{discount}{\\gamma}\n",
    "\\newcommand{qtable}{Q}\n",
    "\\newcommand{finalstate}{\\state_F}\n",
    "%\n",
    "\\newcommand{\\vs}[1]{\\boldsymbol{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\boldsymbol{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\vit{Value Iteration}\n",
    "\\def\\pit{Policy Iteration}\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "\\def\\cstateset{\\mathcal{X}} %%%\n",
    "\\def\\x{\\vs{x}}                    % TODO cstate\n",
    "\\def\\cstate{\\vs{x}}               %%%\n",
    "\\def\\policy{\\pi}\n",
    "\\def\\piparam{\\vs{\\theta}}         % TODO pparam\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\caction{\\vs{u}}       % action\n",
    "\\def\\cactionset{\\mathcal{U}} %%%\n",
    "\\def\\decision{\\vs{d}}       % decision\n",
    "\\def\\randvar{\\vs{\\omega}}       %%%\n",
    "\\def\\randset{\\Omega}       %%%\n",
    "\\def\\transition{T}       %%%\n",
    "\\def\\immediatereward{r}    %%%\n",
    "\\def\\strategichorizon{s}    %%% % TODO\n",
    "\\def\\tacticalhorizon{k}    %%%  % TODO\n",
    "\\def\\operationalhorizon{h}    %%%\n",
    "\\def\\constalpha{a}    %%%\n",
    "\\def\\U{V}              % utility function\n",
    "\\def\\valuefunc{V}\n",
    "\\def\\X{\\mathcal{X}}\n",
    "\\def\\meu{Maximum Expected Utility}\n",
    "\\def\\finaltime{T}\n",
    "\\def\\timeindex{t}\n",
    "\\def\\iterationindex{i}\n",
    "\\def\\decisionfunc{d}       % action\n",
    "\\def\\mdp{\\text{MDP}}\n",
    "$\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In a previous lab, we have dealt with reinforcement learning in discrete state and action spaces.\n",
    "To do so, we used methods based on action-value function and, especially, $Q$-function estimation.\n",
    "The $Q$-function was stored in a table and updated with on- or off- policy algorithms (namely SARSA and $Q$-Learning). \n",
    "\n",
    "Yet, these methods do not scale to large state spaces and especially not to the case of continuous state spaces.\n",
    "To address these issues one can either extend value-based methods making use of value-function approximation or directly search in policy spaces.\n",
    "In this lab, we will explore both solutions. \n",
    "\n",
    "The first part of this lab presents the problem to solve: the CartPole envronment. \n",
    "\n",
    "In the second part, we will search in a family of parameterized policies $\\pi_\\theta(s, a)$ using a policy gradient method.\n",
    "\n",
    "In the third part of this lab, we will apply value-function approximation methods (namely DQN) to solve the CartPole problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Hands on Cart Pole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided by the Gymnasium suite.\n",
    "Gymnasium provides controllable environments (https://gymnasium.farama.org/environments/classic_control/) for research in reinforcement learning.\n",
    "Especially, we will try to solve the CartPole-v1 environment (c.f. https://gymnasium.farama.org/environments/classic_control/cart_pole/) which offers a continuous state space and discrete action space.\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 195 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{\\partial x}{\\partial t},\\omega,\\frac{\\partial \\omega}{\\partial t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Hands on Cart Pole\n",
    "\n",
    "**Task 1:** read https://gymnasium.farama.org/environments/classic_control/cart_pole/ to discover the CartPole environment.\n",
    "\n",
    "**Notice:** A reminder of Gymnasium main concepts is available at https://gymnasium.farama.org/content/basic_usage/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policies (for instance constant actions or randomly drawn actions) to discover the CartPole environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    # TODO\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"rl3_ex1left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    # TODO\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"rl3_ex1right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(5):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(70):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"rl3_ex1random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Implement REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will solve the CartPole environment using a policy gradient method which directly searchs in a family of parameterized policies $\\pi_\\theta(s, a)$ for the optimal policy.\n",
    "This method performs gradient ascent in the policy space so that the total return is maximized.\n",
    "We will restrict our work to episodic tasks, *i.e.* tasks that have a starting states and last for a finite and fixed number of steps $H$, called horizon. \n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\E_{\\pi_\\theta}\\left[\\sum_{t=1}^H r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\E_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(s,.)$ and $H$ is the horizon of the episode.\n",
    "In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$.\n",
    "The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\E_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) ~ Q^{\\pi_\\theta}(s,a) \\right],\n",
    "$$\n",
    "With \n",
    "\n",
    "$$Q^\\pi(s,a) = \\E^\\pi \\left[\\sum_{t=1}^H r(s_t,a_t)|s=s_1, a=a_1\\right].$$\n",
    "\n",
    "Policy Gradient theorem is extremely powerful because it says one doesn't need to know the dynamics of the system to compute the gradient if one can compute the $Q$-function of the current policy.\n",
    "By applying the policy and observing the one-step transitions is enough.\n",
    "Using a stochastic gradient ascent and replacing $Q^{\\pi_\\theta}(s_t,a_t)$ by a Monte Carlo estimate $R_t = \\sum_{t'=t}^H r(s_{t'},a_{t'})$ over one single trajectory, we end up with a special case of the REINFORCE algorithm (see Algorithm below).\n",
    "\n",
    "---\n",
    "REINFORCE with Policy Gradient theorem Algorithm\n",
    "---\n",
    "\n",
    "Initialize $\\theta^0$ as random<br>\n",
    "Initialize step-size $\\alpha_0$<br>\n",
    "$n \\leftarrow 0$<br>\n",
    "<b>WHILE</b> no convergence<br>\n",
    "\t$\\quad$ Generate rollout $h_n \\leftarrow \\{s_1^n,a_1^n,r_1^n, \\ldots, s_H^n, a_H^n, r_H^n\\} \\sim \\pi_{\\theta^n}$<br>\n",
    "\t$\\quad$ $PG_\\theta \\leftarrow 0$<br>\n",
    "\t$\\quad$ <b>FOR</b> $t=1$ to $H$<br>\n",
    "\t\t$\\quad\\quad$ $R_t \\leftarrow \\sum_{t'=t}^H r_{t'}^n$<br>\n",
    "\t\t$\\quad\\quad$ $PG_\\theta \\leftarrow PG_\\theta + \\nabla_\\theta \\log \\pi_{\\theta^{n}}(s_t,a_t) ~ R_t$<br>\n",
    "\t$\\quad$ $n \\leftarrow n + 1$ <br>\n",
    "\t$\\quad$ $\\theta^n \\leftarrow \\theta^{n-1} + \\alpha_n PG_\\theta$<br>\n",
    "\t$\\quad$  update $\\alpha_n$ (if step-size scheduling)<br>\n",
    "\n",
    "<b>RETURN</b> $\\theta^n$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: by replacing the $Q$-function by a Monte-Carlo estimate, we get rid of the Markov assumption and this algorithm is expected to work even in non-Markovian systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Implement a sigmoid policy\n",
    "\n",
    "As the number of actions is $2$ (push the cart to the left or push it to the right), one can see the problem of controlling the Cart Pole as a binary classification problem.\n",
    "Binary classification can be easily solved thanks to logistic regression which transforms the classification problem into a regression problem using the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `sigmoid` function defined by:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Complete the `logistic_regression` function that implements the logistic regression. This function returns the probability to draw action 1 (\"push right\") w.r.t the parameter vector $\\theta$ and the input vector $s$ (the 4-dimension state vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_logistic_regression_fig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(s, theta):\n",
    "    prob_push_right = ... # TODO\n",
    "\n",
    "    return prob_push_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Complete the `draw_action` function that draw an action according to current policy i.e. that select the *RIGHT* action with probability $\\sigma(\\theta^\\top s)$, where $\\theta$ is the parameter vector and $s$ is the 4-dimension state vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_action(s, theta):\n",
    "    prob_push_right = logistic_regression(s, theta)\n",
    "\n",
    "    return ... # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compute $\\nabla_{\\theta} \\log \\pi_\\theta (s,a)$\n",
    "\n",
    "Verify that, for a sigmoid policy:\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{RIGHT}) = \\pi_\\theta (s, \\text{LEFT}) \\times s$\n",
    "- $\\nabla_\\theta \\log \\pi_\\theta (s,\\text{LEFT}) = - \\pi_\\theta (s, \\text{RIGHT}) \\times s$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Implement REINFORCE\n",
    "\n",
    "Fill the following cells to implement the REINFORCE algorithm (defined in the introduction of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"CartPole-v1\"\n",
    "\n",
    "# Since the goal is to attain an average return of 195, horizon should be larger than 195 steps (say 300 for instance)\n",
    "EPISODE_DURATION = 300\n",
    "\n",
    "ALPHA_INIT = 0.1\n",
    "SCORE = 195.0\n",
    "NUM_EPISODES = 100\n",
    "LEFT = 0\n",
    "RIGHT = 1\n",
    "\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `play_one_episode` function that plays an episode with the given policy $\\pi_\\theta$ (for fixed horizon $H$) and returns its rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an episode\n",
    "def play_one_episode(env, theta, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    s_t, info = env.reset()\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(s_t)\n",
    "\n",
    "    for t in range(max_episode_length):\n",
    "\n",
    "        if render:\n",
    "            env.render_wrapper.render()\n",
    "\n",
    "        a_t = ... # TODO\n",
    "        s_t, r_t, done, info = ... # TODO\n",
    "\n",
    "        episode_states.append(s_t)\n",
    "        episode_actions.append(a_t)\n",
    "        episode_rewards.append(r_t)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Implement the `score_on_multiple_episodes` function that test the given policy $\\pi_\\theta$ on `num_episodes` episodes (for fixed horizon $H$) and returns:\n",
    "- `success`: `True` if the agent got an average reward greater or equals to 195 over 100 consecutive trials, `False` otherwise\n",
    "- `num_success`: the number of episodes where the agent got an average reward greater or equals to 195\n",
    "- `average_return`: the average reward on the `num_episodes` episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_on_multiple_episodes(env, theta, score=SCORE, num_episodes=NUM_EPISODES, max_episode_length=EPISODE_DURATION, render=False):\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    return success, num_success, average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Implement the `compute_policy_gradient` function that returns Policy Gradient for a given episode: policy gradient = $\\sum_{t=1}^H \\nabla_\\theta \\log \\pi_\\theta(s_t,a,_t) R_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Policy Gradient for a given episode\n",
    "def compute_policy_gradient(episode_states, episode_actions, episode_rewards, theta):\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return PG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4**: Implement the `train` function that updates $\\theta$ parameters with gradient ascent until the agent got an average reward greater or equals to 195 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent got an average reward greater or equals to 195 over 100 consecutive trials\n",
    "def train(env, theta_init, max_episode_length = EPISODE_DURATION, alpha_init = ALPHA_INIT):\n",
    "\n",
    "    theta = theta_init\n",
    "    episode_index = 0\n",
    "    average_returns = []\n",
    "\n",
    "    success, _, R = score_on_multiple_episodes(env, theta)\n",
    "    average_returns.append(R)\n",
    "\n",
    "    # Train until success\n",
    "    while (not success):\n",
    "\n",
    "        # TODO\n",
    "\n",
    "    return theta, episode_index, average_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1234)\n",
    "env = gym.make(ENV_NAME, render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "#env.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = env.observation_space.shape[0]\n",
    "\n",
    "# Init parameters to random\n",
    "theta_init = np.random.randn(1, dim)\n",
    "\n",
    "# Train the agent\n",
    "theta, i, average_returns = train(env, theta_init)\n",
    "\n",
    "print(\"Solved after {} iterations\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_on_multiple_episodes(env, theta, num_episodes=10, render=True)\n",
    "env.render_wrapper.make_gif(\"rl3_ex4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the evolution of the average reward w.r.t. PG iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show training curve\n",
    "plt.plot(range(len(average_returns)),average_returns)\n",
    "plt.title(\"Average reward on 100 episodes\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Implement DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBd-6HsA29ES"
   },
   "source": [
    "In a previous lab, we have dealt with reinforcement learning in discrete state and action spaces. To do so, we used methods based on action-value function and, especially, Q-function estimation. The Q-function was stored in a table and updated with on- or off- policy algorithms (namely SARSA and Q-Learning).\n",
    "\n",
    "Yet, these methods do not scale to large state spaces. The main idea behind Q-learning is that if we had a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, that could tell us what our return would be, if we were to take an action in a given state, then we could easily construct a policy that maximizes our rewards:\n",
    "\n",
    "\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}\n",
    "\n",
    "However, we don't know everything about the world, so we don't have access to $Q^*$. But, since neural networks are universal function approximators, we can simply create one and train it to resemble $Q^*$. To address this issue, an option is to make use of the power of deep learning to estimate the Q-value of a state-action pair even if it was never encountered before. A deep neural network is used to estimate the Q-value of each possible action in a given state. \n",
    "\n",
    "We recall the update rule for DQN:\n",
    "$$Q(S_t,A_t)\\gets Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma\\max_aQ(S_{t+1},a)-Q(S_t,A_t)\\right]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUOKH1FY3Tp9"
   },
   "source": [
    "---\n",
    "DQN Algorithm\n",
    "---\n",
    "\n",
    "Initialize $\\mathcal{A}$ the agent neural network<br>\n",
    "Copy $\\mathcal{A}$ weights to initialize the target neural network $\\mathcal{T}$<br>\n",
    "Initialize batch_size B<br>\n",
    "<b>FOR</b> i in nb_episodes:<br>\n",
    "    $\\quad$ <b>WHILE</b> not done:<br>\n",
    "    $\\quad$ $\\quad$ Generate experience $exp\\gets[\\{s_1^1,a^1,r^1,s_2^1,done^1,\\dots, s_1^n,a^n,r^n,s_2^n,done^n\\}]$<br>\n",
    "    $\\quad$ $\\quad$ <b>IF</b> len(exp)>B:<br>\n",
    "        $\\quad$ $\\quad$ $\\quad$ sample minibatch $\\sim$ exp<br>\n",
    "        $\\quad$ $\\quad$ $\\quad$ $q_1 \\gets \\mathcal{A}(s_1)^B$<br>\n",
    "        $\\quad$ $\\quad$ $\\quad$ $q_2 \\gets \\mathcal{T}(s_2)^B$<br>\n",
    "        $\\quad$ $\\quad$ $\\quad$ q_target = reward_batch + $\\gamma * q_2$ * (1-done_batch)<br>\n",
    "        $\\quad$ $\\quad$ $\\quad$ Update $\\mathcal{A}$<br>\n",
    "    $\\quad$ $\\quad$ Every j iterations, $\\mathcal{A}$ = $\\mathcal{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Implement DQN\n",
    "\n",
    "Fill the following cells to implement the DQN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import deque\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t11fxc-UUn58"
   },
   "outputs": [],
   "source": [
    "# Here we define some hyperparameters and the agent's architecture\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "observation_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.n\n",
    "\n",
    "EPISODES = 150\n",
    "LR = 0.0001\n",
    "MEM_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.999\n",
    "EXPLORATION_MIN = 0.001\n",
    "sync_freq = 10\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = env.observation_space.shape\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_shape, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "        self.loss = nn.MSELoss()\n",
    "        #self.to(DEVICE)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0G9kMBUsY_5-"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEM_SIZE)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "        state1_batch = torch.stack([s1 for (s1,a,r,s2,d) in minibatch])\n",
    "        action_batch = torch.tensor([a for (s1,a,r,s2,d) in minibatch])\n",
    "        reward_batch = torch.tensor([r for (s1,a,r,s2,d) in minibatch])\n",
    "        state2_batch = torch.stack([s2 for (s1,a,r,s2,d) in minibatch])\n",
    "        done_batch = torch.tensor([d for (s1,a,r,s2,d) in minibatch])\n",
    "\n",
    "        return (state1_batch, action_batch, reward_batch, state2_batch, done_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lWAMRs8X-eI"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.replay = ReplayBuffer()\n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network()\n",
    "        self.network2 = copy.deepcopy(self.network) #A\n",
    "        self.network2.load_state_dict(self.network.state_dict())\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # Convert observation to PyTorch Tensor\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        #state = state.to(DEVICE)\n",
    "        state = state.unsqueeze(0)\n",
    "            \n",
    "        ### BEGIN TODO ###\n",
    "\n",
    "        # Get Q(s,.)\n",
    "        q_values = \n",
    "\n",
    "        # Choose the action to play\n",
    "        action = \n",
    "\n",
    "        ### END TODO ###\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "        if len(self.replay.memory)< BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        ### BEGIN TODO ###\n",
    "\n",
    "        # Sample minibatch s1, a1, r1, s1', done_1, ... , sn, an, rn, sn', done_n\n",
    "        state1_batch, action_batch, reward_batch, state2_batch, done_batch = \n",
    "\n",
    "        # Compute Q values (call self.network and apply the squeeze method on the result)\n",
    "        q_values = \n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute next Q values (call self.network and apply the squeeze method on the result)\n",
    "            next_q_values = \n",
    "\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        predicted_value_of_now = \n",
    "        predicted_value_of_future = \n",
    "\n",
    "        # Compute the q_target\n",
    "        q_target = \n",
    "\n",
    "        # Compute the loss (c.f. self.network.loss())\n",
    "        loss = \n",
    "\n",
    "        ### END TODO ###\n",
    "\n",
    "        # Complute 𝛁Q\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "\n",
    "\n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645,
     "referenced_widgets": [
      "4c92567311114e16b7ca5dc65af2ade7",
      "0f48c897f6f843b88b1b9a4a2ac30e89",
      "548dc19636dd40ca8d93cec4ff756a00",
      "f84565513f8f4d9e925685447eb2744c",
      "ea7146db6fbd46108a46913bca7cd80a",
      "cb112514da704a449d12077e80afcf66",
      "b74c722dcb7347e09c5aade81b7a6573",
      "fb3051f988874649ad3907b0b930f890",
      "dd8eabb50e9b4c0f80e635676c486b80",
      "fb35c6ed7a37409c91571363123df21f",
      "4228ac5540944c7c82ffe9179a4c894b"
     ]
    },
    "id": "tQrkTWktbmQ6",
    "outputId": "146b3a64-e4ef-4fdd-ef7f-d57738303830"
   },
   "outputs": [],
   "source": [
    "agent = DQN()\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "j=0\n",
    "for i in tqdm(range(1, EPISODES)):\n",
    "    state, info = env.reset()\n",
    "    state = np.reshape(state, [1, observation_space])\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        j+=1\n",
    "\n",
    "        action = agent.choose_action(state)\n",
    "        state_, reward, done, truncated, info = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, observation_space])\n",
    "        state = torch.tensor(state).float()\n",
    "        state_ = torch.tensor(state_).float()\n",
    "\n",
    "        exp = (state, action, reward, state_, done)\n",
    "        agent.replay.add(exp)\n",
    "        agent.learn()\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        if j % sync_freq == 0:\n",
    "            agent.network2.load_state_dict(agent.network.state_dict())\n",
    "\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "            average_reward += score \n",
    "            if i%10==0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {}\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon()))\n",
    "                #test_model(agent,10, observation_space)\n",
    "            break\n",
    "  \n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)\n",
    "\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tO9Jk9Nibxaa"
   },
   "outputs": [],
   "source": [
    "def test_model(model, num_episodes, observation_space):\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        G=0\n",
    "        observation, info = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            observation = torch.from_numpy(observation).float()\n",
    "            observation = np.reshape(observation, [1, observation_space])\n",
    "            action = model.choose_action(observation)\n",
    "            observation, reward, done, truncated, info = env.step(action)\n",
    "            G += reward\n",
    "            if done:\n",
    "                print(G)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t3RIBgnccMIM"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "observation = np.reshape(observation, [1, observation_space])\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = agent.choose_action(observation)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    # TODO\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"rl3_ex1left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nHs-bc00XfB"
   },
   "outputs": [],
   "source": [
    "torch.save(agent.network.state_dict(), \"rl3_dqn_cartpole.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
