{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Optimization - CEM / ES\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf581-2023/master/logo.jpg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[INF581-2023](https://moodle.polytechnique.fr/course/view.php?id=14259) Lab session #9\n",
    "\n",
    "2019-2023 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf581-2023/blob/master/lab9_optim_cem.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf581-2023/master?filepath=lab9_optim_cem.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf581-2023/raw/master/lab9_optim_cem.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice**: this notebook requires the following libraries: *Gymnasium*, NumPy, Pandas, Seaborn, imageio, Box2D, box2d-py, cma, pygame, ipywidgets, ipython and tqdm.\n",
    "\n",
    "You can install them with the following command (the next cells do this for you if you use the Google Colab environment):\n",
    "\n",
    "``\n",
    "pip install gymnasium[box2d] numpy pandas seaborn imageio Box2D box2d-py cma ipython ipywidgets pygame tqdm\n",
    "``\n",
    "\n",
    "C.f. https://gymnasium.farama.org/environments/box2d/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    from inf581 import *\n",
    "except ModuleNotFoundError:\n",
    "    process = subprocess.Popen(\"pip install inf581\".split(), stdout=subprocess.PIPE)\n",
    "\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "    from inf581 import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import json\n",
    "\n",
    "from IPython.display import Image   # To display GIF images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\newcommand{\\vs}[1]{\\mathbf{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\mathbf{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\U{V}\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "%\n",
    "\\def\\E{\\mathbb{E}}\n",
    "%\\newcommand{transition}{T(s,a,s')}\n",
    "%\\newcommand{transitionfunc}{\\mathcal{T}^a_{ss'}}\n",
    "\\newcommand{transitionfunc}{P}\n",
    "\\newcommand{transitionfuncinst}{P(\\nextstate|\\state,\\action)}\n",
    "\\newcommand{transitionfuncpi}{\\mathcal{T}^{\\pi_i(s)}_{ss'}}\n",
    "\\newcommand{rewardfunc}{r}\n",
    "\\newcommand{rewardfuncinst}{r(\\state,\\action,\\nextstate)}\n",
    "\\newcommand{rewardfuncpi}{r(s,\\pi_i(s),s')}\n",
    "\\newcommand{statespace}{\\mathcal{S}}\n",
    "\\newcommand{statespaceterm}{\\mathcal{S}^F}\n",
    "\\newcommand{statespacefull}{\\mathcal{S^+}}\n",
    "\\newcommand{actionspace}{\\mathcal{A}}\n",
    "\\newcommand{reward}{R}\n",
    "\\newcommand{statet}{S}\n",
    "\\newcommand{actiont}{A}\n",
    "\\newcommand{newstatet}{S'}\n",
    "\\newcommand{nextstate}{\\state'}\n",
    "\\newcommand{newactiont}{A'}\n",
    "\\newcommand{stepsize}{\\alpha}\n",
    "\\newcommand{discount}{\\gamma}\n",
    "\\newcommand{qtable}{Q}\n",
    "\\newcommand{finalstate}{\\state_F}\n",
    "%\n",
    "\\newcommand{\\vs}[1]{\\boldsymbol{#1}} % vector symbol (\\boldsymbol, \\textbf or \\vec)\n",
    "\\newcommand{\\ms}[1]{\\boldsymbol{#1}} % matrix symbol (\\boldsymbol, \\textbf)\n",
    "\\def\\vit{Value Iteration}\n",
    "\\def\\pit{Policy Iteration}\n",
    "\\def\\discount{\\gamma}  % discount factor\n",
    "\\def\\state{\\vs{s}}         % state\n",
    "\\def\\S{\\mathcal{S}}         % TODO\n",
    "\\def\\stateset{\\mathcal{S}}  %%%\n",
    "\\def\\cstateset{\\mathcal{X}} %%%\n",
    "\\def\\x{\\vs{x}}                    % TODO cstate\n",
    "\\def\\cstate{\\vs{x}}               %%%\n",
    "\\def\\policy{\\pi}\n",
    "\\def\\piparam{\\vs{\\theta}}         % TODO pparam\n",
    "\\def\\action{\\vs{a}}       % action\n",
    "\\def\\A{\\mathcal{A}}        % TODO\n",
    "\\def\\actionset{\\mathcal{A}} %%%\n",
    "\\def\\caction{\\vs{u}}       % action\n",
    "\\def\\cactionset{\\mathcal{U}} %%%\n",
    "\\def\\decision{\\vs{d}}       % decision\n",
    "\\def\\randvar{\\vs{\\omega}}       %%%\n",
    "\\def\\randset{\\Omega}       %%%\n",
    "\\def\\transition{T}       %%%\n",
    "\\def\\immediatereward{r}    %%%\n",
    "\\def\\strategichorizon{s}    %%% % TODO\n",
    "\\def\\tacticalhorizon{k}    %%%  % TODO\n",
    "\\def\\operationalhorizon{h}    %%%\n",
    "\\def\\constalpha{a}    %%%\n",
    "\\def\\U{V}              % utility function\n",
    "\\def\\valuefunc{V}\n",
    "\\def\\X{\\mathcal{X}}\n",
    "\\def\\meu{Maximum Expected Utility}\n",
    "\\def\\finaltime{T}\n",
    "\\def\\timeindex{t}\n",
    "\\def\\iterationindex{i}\n",
    "\\def\\decisionfunc{d}       % action\n",
    "\\def\\mdp{\\text{MDP}}\n",
    "$\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous lab we studied a method that allowed us to apply reinforcement learning in continuous state spaces and/or continuous action spaces.\n",
    "We used REINFORCE, a *Policy gradient* method that directly optimize the parametric policy $\\pi_{\\theta}$.\n",
    "The parameter $\\theta$ was iteratively updated toward a local maximum of the total expected reward $J(\\theta)$ using a gradient ascent method:\n",
    "$$\\theta \\leftarrow \\theta + \\alpha \\nabla_{\\theta}J(\\theta)$$\n",
    "A convenient analytical formulation of $\\nabla_{\\theta}J(\\theta)$ was obtained thanks to the *Policy Gradient theorem*:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\E_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (s,a) Q^{\\pi_\\theta}(s,a) \\right].$$\n",
    "However, gradient ascent methods may have a slow convergence and will only found a local optimum.\n",
    "Moreover, this approach requires an analytical formulation of $\\nabla_\\theta \\log \\pi_\\theta (s,a)$ which is not always known (when something else than a neural networks is used for the agent's policy).\n",
    "\n",
    "Direct Policy Search methods using gradient free optimization procedures like Evolution Strategies or Cross Entropy Method (CEM) are interesting alternatives to Policy Gradient algorithms.\n",
    "They can be successfully applied as long as the $\\pi_\\theta$ policy has no more than few hundreds of parameters.\n",
    "Moreover, these method can solve complex problems that cannot be modeled as Markov Decision Processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for previous Reinforcement Learning labs, we will use standard problems provided by Gymnasium suite.\n",
    "Especially, we will try to solve the LunarLander-v2 problem (https://gymnasium.farama.org/environments/box2d/lunar_lander/) which offers both continuous space and action states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement CEM and test it on the CartPole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before solving the Lunar Lander environment, we will practice on the (simpler) CartPole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder**: a description of the CartPole environment is available at https://gymnasium.farama.org/environments/classic_control/cart_pole/. This environment offers a continuous state space and discrete action space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of the CartPole policy is given in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "def logistic_regression(s, theta):\n",
    "    prob_push_right = sigmoid(np.dot(s, np.transpose(theta)))\n",
    "    return prob_push_right\n",
    "\n",
    "\n",
    "def draw_action(s, theta):\n",
    "    prob_push_right = logistic_regression(s, theta)\n",
    "    r = np.random.rand()\n",
    "    return 1 if r < prob_push_right else 0\n",
    "\n",
    "\n",
    "# Logistic Regression ############################################\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, env):        \n",
    "        self.num_params = env.observation_space.shape[0]\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        return draw_action(state, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization algorithms aim to find the minimum of a function. This function is called an \"objective function\".\n",
    "The cell below implements the framework for such a function.\n",
    "Note that in reinforcement learning, by convention the score is a reward to maximize whereas in mathematical optimization the score is a cost to minimize (most optimization libraries like PyCMA used in this lab impose this convention) ; the objective function will therefore return the opposite of the reward as the score of evaluated policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "\n",
    "    def __init__(self, env, policy, num_episodes=1, max_time_steps=float('inf'), minimization_solver=True):\n",
    "        self.ndim = policy.num_params  # Number of dimensions of the parameter (weights) space\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "\n",
    "        self.num_evals = 0\n",
    "\n",
    "        \n",
    "    def eval(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
    "        \"\"\"Evaluate a policy\"\"\"\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        average_total_rewards = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "\n",
    "            total_rewards = 0.\n",
    "            state, info = self.env.reset()\n",
    "\n",
    "            for t in range(max_time_steps):\n",
    "                if render:\n",
    "                    self.env.render_wrapper.render()\n",
    "\n",
    "                action = self.policy(state, policy_params)\n",
    "                state, reward, done, truncated, info = self.env.step(action)\n",
    "                total_rewards += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            average_total_rewards += float(total_rewards) / num_episodes\n",
    "\n",
    "            if render:\n",
    "                print(\"Test Episode {0}: Total Reward = {1}\".format(i_episode, total_rewards))\n",
    "\n",
    "        if self.minimization_solver:\n",
    "            average_total_rewards *= -1.\n",
    "\n",
    "        return average_total_rewards   # Optimizers do minimization by default...\n",
    "\n",
    "    \n",
    "    def __call__(self, policy_params, num_episodes=None, max_time_steps=None, render=False):\n",
    "        return self.eval(policy_params, num_episodes, max_time_steps, render)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `cem_uncorrelated` function that search the best $\\theta$ parameters with a Cross Entropy Method. Use the objective function defined above.\n",
    "$\\mathbb{P}$ can be defined as an multivariate normal distribution $\\mathcal{N}\\left( \\boldsymbol{\\mu}, \\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma} \\right)$ where $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\sigma^2} \\boldsymbol{\\Sigma}$ are vectors i.e. we use one mean and one variance parameters per dimension of $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Entropy**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\mathbb{P}$: family of distribution<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta}$: initial parameters for the proposal distribution $\\mathbb{P}$<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $m$: sample size<br>\n",
    "$\\quad\\quad$ $m_{\\text{elite}}$: number of samples to use to fit $\\boldsymbol{\\theta}$<br>\n",
    "\n",
    "**FOR EACH** iteration<br>\n",
    "$\\quad\\quad$ samples $\\leftarrow \\{ \\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_m \\}$ with $\\boldsymbol{x}_i \\sim \\mathbb{P}(\\boldsymbol{\\theta}) ~~ \\forall i \\in 1\\dots m$<br>\n",
    "$\\quad\\quad$ elite $\\leftarrow $ { $m_{\\text{elite}}$ best samples } $\\quad$ (i.e. select best samples according to $f$)<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow $ fit $\\mathbb{P}(\\boldsymbol{\\theta})$ to the elite samples<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_uncorrelated(objective_function,\n",
    "                     mean_array,\n",
    "                     var_array,\n",
    "                     max_iterations=500,\n",
    "                     sample_size=50,\n",
    "                     elite_frac=0.2,\n",
    "                     print_every=10,\n",
    "                     success_score=float(\"inf\"),\n",
    "                     num_evals_for_stop=None,\n",
    "                     hist_dict=None):\n",
    "    \"\"\"Cross-entropy method.\n",
    "        \n",
    "    Params\n",
    "    ======\n",
    "        objective_function (function): the function to maximize\n",
    "        mean_array (array of floats): the initial proposal distribution (mean vector)\n",
    "        var_array (array of floats): the initial proposal distribution (variance vector)\n",
    "        max_iterations (int): number of training iterations\n",
    "        sample_size (int): size of population at each iteration\n",
    "        elite_frac (float): rate of top performers to use in update with elite_frac ∈ ]0;1]\n",
    "        print_every (int): how often to print average score\n",
    "        hist_dict (dict): logs\n",
    "    \"\"\"\n",
    "    assert 0. < elite_frac <= 1.\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    # TODO...\n",
    "  \n",
    "    return mean_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** train your implementation using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = LogisticRegression(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,\n",
    "                                       max_time_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "init_mean_array = np.random.random(nn_policy.num_params)\n",
    "init_var_array = np.ones(nn_policy.num_params) * 100.\n",
    "\n",
    "theta = cem_uncorrelated(objective_function=objective_function,\n",
    "                         mean_array=init_mean_array,\n",
    "                         var_array=init_var_array,\n",
    "                         max_iterations=30,\n",
    "                         sample_size=50,\n",
    "                         elite_frac=0.1,\n",
    "                         print_every=1,\n",
    "                         success_score=-500,\n",
    "                         num_evals_for_stop=None,\n",
    "                         hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index', columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"var1\", \"var2\", \"var3\", \"var4\"])\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(title=\"Theta w.r.t training steps\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"var1\", \"var2\", \"var3\", \"var4\"]].plot(logy=True, title=\"Variance w.r.t training steps\", figsize=(20, 5))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=200, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab9_ex1_cem_cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: implement SAES and solve the CartPole problem with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Implement the `saes_1_1` function that search the best $\\theta$ parameters with a (1+1)-SA-ES algorithm. Use the objective function defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1+1)-SA-ES**\n",
    "\n",
    "**Input**:<br>\n",
    "$\\quad\\quad$ $f$: the objective function<br>\n",
    "$\\quad\\quad$ $\\boldsymbol{x}$: initial solution<br>\n",
    "\n",
    "**Algorithm parameter**:<br>\n",
    "$\\quad\\quad$ $\\tau$: self-adaptation learning rate<br>\n",
    "\n",
    "**FOR EACH** generation<br>\n",
    "$\\quad\\quad$ 1. mutation of $\\sigma$ (current individual strategy) : $\\sigma' \\leftarrow \\sigma ~ e^{\\tau \\mathcal{N}(0,1)}$<br>\n",
    "$\\quad\\quad$ 2. mutation of $\\boldsymbol{x}$ (current solution) : $\\boldsymbol{x}' \\leftarrow \\boldsymbol{x} + \\sigma' ~ \\mathcal{N}(0,1)$<br>\n",
    "$\\quad\\quad$ 3. eval $f(\\boldsymbol{x}')$<br>\n",
    "$\\quad\\quad$ 4. survivor selection $\\boldsymbol{x} \\leftarrow \\boldsymbol{x}'$ and $\\sigma \\leftarrow \\sigma'$ if $f(\\boldsymbol{x}') \\leq f(\\boldsymbol{x})$<br>\n",
    "\n",
    "**RETURN** $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saes_1_1(objective_function,\n",
    "             x_array,\n",
    "             sigma_array,\n",
    "             max_iterations=500,\n",
    "             tau=None,\n",
    "             print_every=10,\n",
    "             success_score=float(\"inf\"),\n",
    "             num_evals_for_stop=None,\n",
    "             hist_dict=None):\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return x_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** train your implementation using the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = LogisticRegression(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,\n",
    "                                       max_time_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "initial_solution_array = np.random.random(nn_policy.num_params)\n",
    "initial_sigma_array = np.ones(nn_policy.num_params) * 1.\n",
    "\n",
    "theta = saes_1_1(objective_function=objective_function,\n",
    "                 x_array=initial_solution_array,\n",
    "                 sigma_array=initial_sigma_array,\n",
    "                 tau = 0.001,\n",
    "                 max_iterations=1000,\n",
    "                 print_every=100,\n",
    "                 success_score=-500,\n",
    "                 num_evals_for_stop=None,\n",
    "                 hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index', columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"])\n",
    "ax = df.score.plot(title=\"Average reward\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"mu1\", \"mu2\", \"mu3\", \"mu4\"]].plot(title=\"Theta w.r.t training steps\", figsize=(30, 5));\n",
    "plt.xlabel(\"Training Steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df[[\"sigma1\", \"sigma2\", \"sigma3\", \"sigma4\"]].plot(logy=True, title=\"Sigma w.r.t training steps\", figsize=(30, 5))\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** test the optimized policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=200, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab9_ex2_saes_cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** try different values of $\\tau$. What happen ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Implement a parametric policy $\\pi_\\theta$ for environments having a continuous action space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve problems having a continuous space, especially to solve the LunarLander problem in the next exercise, we need to define and implement an appropriate parametric policy.\n",
    "For this purpose, we recommend the following neural network:\n",
    "- one hidden layer of 16 units having a ReLu activation function\n",
    "- a tanh activation function on the output layer (be careful on the number of output units)\n",
    "\n",
    "To solve environments with continuous action space like LunarLander with Direct Policy Search methods, a simple procedure that compute the feed forward signal is needed (we don't do back propagation here).\n",
    "A procedure to set/get weights of the network from/to a single vector $\\theta$ will also be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions ########################################################\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    x_and_zeros = np.array([x, np.zeros(x.shape)])\n",
    "    return np.max(x_and_zeros, axis=0)\n",
    "\n",
    "# Dense Multi-Layer Neural Network ############################################\n",
    "\n",
    "class NeuralNetworkPolicy:\n",
    "\n",
    "    def __init__(self, env, h_size=16):   # h_size = number of neurons on the hidden layer\n",
    "        # Set the neural network activation functions (one function per layer)\n",
    "        self.activation_functions = (relu, tanh)\n",
    "        \n",
    "        # Make a neural network with 1 hidden layer of `h_size` units\n",
    "        weights = (np.zeros([env.observation_space.shape[0] + 1, h_size]),\n",
    "                   np.zeros([h_size + 1, env.action_space.shape[0]]))\n",
    "\n",
    "        self.shape_list = weights_shape(weights)\n",
    "        print(\"Number of parameters per layer:\", self.shape_list)\n",
    "        \n",
    "        self.num_params = len(flatten_weights(weights))\n",
    "        print(\"Number of parameters (neural network weights) to optimize:\", self.num_params)\n",
    "\n",
    "\n",
    "    def __call__(self, state, theta):\n",
    "        weights = unflatten_weights(theta, self.shape_list)\n",
    "\n",
    "        return feed_forward(inputs=state,\n",
    "                            weights=weights,\n",
    "                            activation_functions=self.activation_functions)\n",
    "\n",
    "\n",
    "def feed_forward(inputs, weights, activation_functions, verbose=False):\n",
    "\n",
    "    # TODO...\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "\n",
    "def weights_shape(weights):\n",
    "    return [weights_array.shape for weights_array in weights]\n",
    "\n",
    "\n",
    "def flatten_weights(weights):\n",
    "    \"\"\"Convert weight parameters to a 1 dimension array (more convenient for optimization algorithms)\"\"\"\n",
    "    nested_list = [weights_2d_array.flatten().tolist() for weights_2d_array in weights]\n",
    "    flat_list = list(itertools.chain(*nested_list))\n",
    "    return flat_list\n",
    "\n",
    "\n",
    "def unflatten_weights(flat_list, shape_list):\n",
    "    \"\"\"The reverse function of `flatten_weights`\"\"\"\n",
    "    length_list = [shape[0] * shape[1] for shape in shape_list]\n",
    "\n",
    "    nested_list = []\n",
    "    start_index = 0\n",
    "\n",
    "    for length, shape in zip(length_list, shape_list):\n",
    "        nested_list.append(np.array(flat_list[start_index:start_index+length]).reshape(shape))\n",
    "        start_index += length\n",
    "\n",
    "    return nested_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: solve the LunarLander problem (continuous version) with CEM and SAES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** read https://gymnasium.farama.org/environments/box2d/lunar_lander/ to discover the LunarLanderContinuous environment.\n",
    "\n",
    "**Notice:** A reminder of Gymnasium main concepts is available at https://gymnasium.farama.org/content/basic_usage/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "print(\"State space dimension is:\", env.observation_space.shape[0])\n",
    "print(\"State upper bounds:\", env.observation_space.high)\n",
    "print(\"State lower bounds:\", env.observation_space.low)\n",
    "print(\"Actions upper bounds:\", env.action_space.high)\n",
    "print(\"Actions lower bounds:\", env.action_space.low)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic policies (for instance constant actions or randomly drawn actions) to discover the Lunar Lander environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(150):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    #action = np.array([1., 1.])\n",
    "    action = np.array([-1., -1.])\n",
    "    #action = env.action_space.sample()   # Random policy\n",
    "    \n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(\"lab9_ex4_explore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** We want to use CEM and SAES to compute the optimal policy for the Lunar Lander environment.\n",
    "What is the size of the search space (number of dimensions) for optimizers knowing that the policy is the one defined in exercise 3 (a neural network of one hidden layer of 16 neurons) and knowing that the State space of the Lunar Lander environment is $\\mathcal{S} = \\mathbb{R}^8$ and its action space is $\\mathcal{A} \\subset \\mathbb{R}^2$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Train the agent using the CEM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=5,\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "init_mean_array = np.random.random(nn_policy.num_params)\n",
    "init_var_array = np.ones(nn_policy.num_params) * 1000.\n",
    "\n",
    "theta = cem_uncorrelated(objective_function=objective_function,\n",
    "                         mean_array=init_mean_array,\n",
    "                         var_array=init_var_array,\n",
    "                         max_iterations=100,\n",
    "                         sample_size=50,\n",
    "                         elite_frac=0.2,\n",
    "                         print_every=1,\n",
    "                         success_score=-200,\n",
    "                         hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index')\n",
    "ax = df.iloc[:,0].plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,1:66].plot(title=\"Theta w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,67:].plot(logy=True, title=\"Variance w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** check the optimized policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=500, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab9_ex4_cem_ll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** Train the agent using the (1+1)-SA-ES algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=10,     # <- resampling\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hist_dict = {}\n",
    "\n",
    "initial_solution_array = np.random.random(nn_policy.num_params)\n",
    "initial_sigma_array = np.ones(nn_policy.num_params) * 1.\n",
    "\n",
    "theta = saes_1_1(objective_function=objective_function,\n",
    "                 x_array=initial_solution_array,\n",
    "                 sigma_array=initial_sigma_array,\n",
    "                 max_iterations=1000,\n",
    "                 print_every=10,\n",
    "                 success_score=-200,\n",
    "                 num_evals_for_stop=None,\n",
    "                 hist_dict=hist_dict)\n",
    "\n",
    "objective_function.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(hist_dict, orient='index')\n",
    "ax = df.iloc[:,0].plot(title=\"Average reward\", figsize=(20, 5));\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Reward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,1:66].plot(title=\"Theta w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.iloc[:,67:].plot(logy=True, title=\"Variance w.r.t training steps\", legend=None, figsize=(20, 10))\n",
    "#ax.get_legend().remove()\n",
    "plt.xlabel(\"Training Steps\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimized weights: \", theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6:** check the optimized policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "objective_function.eval(theta, num_episodes=3, max_time_steps=500, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab9_ex4_saes_ll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise 1: implement CEM with an alternative *Proposal distribution*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement CEM with the following alternative *Proposal distribution*: a multivariate normal distribution parametrized by a mean vector and **a covariance matrix** (to use correlations in the search space dimensions).\n",
    "\n",
    "Test it on the CartPole and the LunarLander environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus exercise 2: test the CMAES algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyCMA: Python implementation of CMA-ES (from Nikolaus Hansen - CMAP).\n",
    "\n",
    "Source code:\n",
    "\n",
    "- http://cma.gforge.inria.fr/cmaes_sourcecode_page.html#python\n",
    "- https://github.com/CMA-ES/pycma\n",
    "- https://pypi.org/project/cma/\n",
    "\n",
    "Official documentation:\n",
    "\n",
    "- http://cma.gforge.inria.fr/apidocs-pycma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", continuous=True, render_mode=\"rgb_array\")\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "nn_policy = NeuralNetworkPolicy(env)\n",
    "\n",
    "objective_function = ObjectiveFunction(env=env,\n",
    "                                       policy=nn_policy,\n",
    "                                       num_episodes=1,\n",
    "                                       max_time_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "x_optimal, es = cma.fmin2(objective_function, x0=np.random.random(nn_policy.num_params), sigma0=10., options={'maxfevals': 1500})\n",
    "theta = x_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")\n",
    "plt.rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "cma.plot();  # shortcut for es.logger.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function.eval(theta, num_episodes=3, render=True)\n",
    "\n",
    "objective_function.env.close()\n",
    "\n",
    "objective_function.env.render_wrapper.make_gif(\"lab9_ex_pycma_ll\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
